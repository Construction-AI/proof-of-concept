services:
  # LlamaIndex service for document processing and RAG
  llamaindex:
    build:
      context: ./llamaindex
      dockerfile: Dockerfile
    ports:
      - "8000:8000"
    volumes:
      - ../:/workspaces:cached
      - ./llamaindex:/app
      - uploaded_docs:/data/documents:ro
      - index_data:/data/indexes
    environment:
      - OLLAMA_BASE_URL=${OLLAMA_BASE_URL}
      - QDRANT_URL=${QDRANT_URL}
      - QUERY_MODEL=${QUERY_MODEL}
      - EMBEDDINGS_MODEL=${EMBEDDINGS_MODEL}
      - OLLAMA_TIMEOUT=${OLLAMA_TIMEOUT}
    depends_on:
      - qdrant
      - ollama
    networks:
      - app-network

    # Streamlit web interface
  frontend:
    build:
      context: ./frontend
      dockerfile: Dockerfile
    ports:
      - "8501:8501"
    volumes:
      - ./frontend:/app
      - uploaded_docs:/data/documents
    depends_on:
      - llamaindex
    environment:
      - LLAMAINDEX_BASE_URL=${LLAMAINDEX_BASE_URL}
    networks:
      - app-network


  # Vector database
  qdrant:
    image: qdrant/qdrant
    ports:
      - "6333:6333"
    networks:
      - app-network

  # LLM service
  ollama:
    build:
      context: ./ollama
      dockerfile: Dockerfile
    volumes:
      - ollama:/root/.ollama
    ports:
      - "11434:11434"
    networks:
      - app-network
    environment:
      - QUERY_MODEL=${QUERY_MODEL}
      - EMBEDDINGS_MODEL=${EMBEDDINGS_MODEL}

volumes:
  uploaded_docs:
  index_data:
  ollama:

networks:
  app-network:
    driver: bridge